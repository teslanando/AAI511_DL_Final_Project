{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pretty_midi\n",
    "from music21 import converter, chord\n",
    "from tqdm import tqdm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection | Pre-Processing | Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Bach files:  57%|█████▋    | 495/876 [02:56<06:18,  1.01it/s]"
     ]
    }
   ],
   "source": [
    "# Path to the dataset folder\n",
    "MIDI_FOLDER = \"./dataset\" \n",
    "CSV_FOLDER = \"./features_csv\"\n",
    "\n",
    "# Ensure the CSV folder exists\n",
    "os.makedirs(CSV_FOLDER, exist_ok=True)\n",
    "\n",
    "def extract_features(midi_path):\n",
    "    try:\n",
    "        midi_data = pretty_midi.PrettyMIDI(midi_path)\n",
    "        features = {}\n",
    "\n",
    "        # Extract global features like tempo and number of instruments\n",
    "        features['tempo'] = np.mean(midi_data.get_tempo_changes()[1])\n",
    "        features['num_instruments'] = len(midi_data.instruments)\n",
    "\n",
    "        # Extract note-level features for each instrument\n",
    "        notes = []\n",
    "        for instrument in midi_data.instruments:\n",
    "            for note in instrument.notes:\n",
    "                notes.append((note.pitch, note.start, note.end, note.velocity))\n",
    "\n",
    "        # Convert notes to a DataFrame to facilitate further analysis\n",
    "        if notes:\n",
    "            df_notes = pd.DataFrame(notes, columns=['Pitch', 'Start', 'End', 'Velocity'])\n",
    "            features['avg_pitch'] = df_notes['Pitch'].mean()\n",
    "            features['std_pitch'] = df_notes['Pitch'].std()\n",
    "            features['avg_velocity'] = df_notes['Velocity'].mean()\n",
    "            features['std_velocity'] = df_notes['Velocity'].std()\n",
    "\n",
    "        # Analyze chords using music21\n",
    "        score = converter.parse(midi_path)\n",
    "        chords = [ch for ch in score.chordify().recurse() if isinstance(ch, chord.Chord)]\n",
    "        pitches = [p for ch in chords for p in ch.pitches]\n",
    "        features['num_chords'] = len(chords)\n",
    "        features['unique_pitches'] = len(set(pitches))\n",
    "\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {midi_path}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def process_files():\n",
    "    csv_path = os.path.join(CSV_FOLDER, 'features.csv')\n",
    "    composers = ['Bach', 'Beethoven', 'Chopin', 'Mozart']\n",
    "    all_features = []\n",
    "\n",
    "    for composer in composers:\n",
    "        composer_path = os.path.join(MIDI_FOLDER, composer)\n",
    "        midi_files = [f for f in os.listdir(composer_path) if f.endswith('.mid')]\n",
    "\n",
    "        for filename in tqdm(midi_files, desc=f\"Processing {composer} files\"):\n",
    "            midi_path = os.path.join(composer_path, filename)\n",
    "            features = extract_features(midi_path)\n",
    "            if features:\n",
    "                features['filename'] = filename\n",
    "                features['composer'] = composer\n",
    "                all_features.append(features)\n",
    "\n",
    "    # Save all features to a CSV file\n",
    "    df = pd.DataFrame(all_features)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(\"Features saved to:\", csv_path)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    process_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(network_input.shape[1], 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the checkpoint\n",
    "checkpoint = ModelCheckpoint(\"best_model.hdf5\", monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model.fit(network_input, network_output, epochs=50, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Split data into training and test sets\n",
    "input_train, input_test, output_train, output_test = train_test_split(network_input, network_output, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(input_train, output_train, epochs=50, batch_size=64, validation_data=(input_test, output_test), callbacks=callbacks_list)\n",
    "\n",
    "# Load the best saved model\n",
    "best_model = load_model('best_model.hdf5')\n",
    "\n",
    "# Predictions\n",
    "predictions = best_model.predict(input_test, verbose=1)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(output_test, axis=1)\n",
    "\n",
    "# Evaluation metrics\n",
    "print(\"Accuracy: \", accuracy_score(true_classes, predicted_classes))\n",
    "print(classification_report(true_classes, predicted_classes, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(network_input.shape[1], 1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Wrap Keras model so it can be used by scikit-learn\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# Define the grid search parameters\n",
    "batch_size = [32, 64, 128]\n",
    "epochs = [30, 50, 100]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(network_input, network_output)\n",
    "\n",
    "# Summary\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(true_classes, predicted_classes)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Classes')\n",
    "plt.xlabel('Predicted Classes')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
